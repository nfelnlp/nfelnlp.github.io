---
layout: single
classes: wide
author_profile: true
---

# Supervision

## Open topics

InquAIrer: Conversational Model Refinement from Eliciting Expert Human Feedback using Targeted Question Generation  
<p style="font-size:smaller;">References: <a href="https://arxiv.org/abs/2103.10415">Yao et al. (2021)</a>; <a href="https://arxiv.org/abs/2311.09613">Gu et al. (2023)</a>; <a href="https://arxiv.org/abs/2311.09558">Malaviya et al. (2023)</a>; <a href="https://aclanthology.org/2023.acl-long.474/">He et al. (2023)</a></p>  

Analyzing User Behavior in Fact Checking Systems with Retrieval- and Tool-augmented Generation  
<p style="font-size:smaller;">References: <a href="https://arxiv.org/abs/2310.12558">Si et al. (2023)</a>; <a href="https://doi.org/10.1609/icwsm.v15i1.18072">Mohseni et al. (2021)</a>; <a href="https://doi.org/10.1002/ail2.49">Linder et al. (2021)</a></p>  

LLM-based Evaluation of Instructional Explanations on Different Expertise Levels  
<p style="font-size:smaller;">References: <a href="https://aclanthology.org/2022.coling-1.27/">Wachsmuth & Alshomary (2022)</a>; <a href="http://arxiv.org/abs/2311.10749">Kupor et al. (2023)</a>; <a href="https://dl.acm.org/doi/10.1145/3544548.3581369">Lee et al. (2023)</a>; <a href="https://arxiv.org/abs/2312.02065">Rooein et al. (2023)</a></p>  

Efficiently Evaluating the Faithfulness of Free-text Rationales  
<p style="font-size:smaller;">References: <a href="http://arxiv.org/abs/2311.07466">Parcalabescu & Frank (2023)</a>; <a href="https://aclanthology.org/2023.findings-emnlp.7/">Larionov et al. (2023)</a>; <a href="https://aclanthology.org/2021.blackboxnlp-1.17/">Schwarzenberg et al. (2021)</a></p>  

Explaining Disagreements in Automated Text Simplification Evaluation  
<p style="font-size:smaller;">References: <a href="https://arxiv.org/abs/2310.00752">Jiang et al. (2023)</a>; <a href="https://aclanthology.org/2023.emnlp-main.714/">Ribeiro et al. (2023)</a>; <a href="http://arxiv.org/abs/2305.14770">Wadhwa et al. (2023)</a>; <a href="https://aclanthology.org/2023.acl-long.674/">He et al. (2023)</a></p>  

Synthesizing Training Data from Human Feedback to Natural Language Explanations  
<p style="font-size:smaller;">References: <a href="https://aclanthology.org/2022.findings-acl.75/">Li et al. (2022)</a>; <a href="https://aclanthology.org/2023.findings-emnlp.791/">Wang et al. (2023)</a>; <a href="https://aclanthology.org/2022.findings-emnlp.269/">Ye et al. (2022)</a></p>  



## Ongoing
Maximilian Bleick (with [Aljoscha Burchardt](https://www.dfki.de/~aburch/)) – BA thesis @ TU Berlin: [An Investigation of LLM Chatbots Concerning the Echo Chamber Effect](https://tu.berlin/index.php?id=246820)  

Yi-Sheng Hsu (with [Sherzod Hakimov](https://sherzod-hakimov.github.io/)) – MSc project @ Uni Potsdam: Engineering LLM-generated Explanations with Metric-based Readability Control  


## Completed
[Qianli Wang](https://github.com/qiaw99) (with [Leonhard Hennig](https://dfki-nlp.github.io/authors/leonhard-hennig/)) – MSC thesis @ TU Berlin: A Singular LLM Is All You Need for Dialogue-based Explanation Regarding NLP Tasks  

Konstantin Biskupski (with [Eleftherios Avramidis](https://github.com/lefterav)) – MSc thesis @ TU Berlin: Quality estimation of machine-translated texts with fine-grained classification of errors  

Kiran Rohra (with [Philippe Thomas](https://github.com/erechtheus)) – MSc thesis @ TU Berlin: Comparative error analysis of biomedical image labelling and captioning models  

[Ajay Madhavan Ravichandran](https://github.com/aj280192) (with [Philippe Thomas](https://github.com/erechtheus)) – MSc thesis @ TU Berlin: Evaluating text quality of generated radiology reports  

Mika Rebensburg (with [Tim Polzehl](https://www.tu.berlin/en/qu/ueber-uns/team-personen/gast-wissenschaftler-partner/dr-tim-polzehl) & [Stefan Hillmann](https://www.tu.berlin/index.php?id=29495)) - BSc thesis @ TU Berlin : Automatic Evaluation of Chatbot Dialogs Using Pre-Trained Language Models in the Customer Support Domain  

Daniel Fernau (with [Tim Polzehl](https://www.tu.berlin/en/qu/ueber-uns/team-personen/gast-wissenschaftler-partner/dr-tim-polzehl) & [Stefan Hillmann](https://www.tu.berlin/index.php?id=29495)) – MSc thesis @ TU Berlin: Towards Adaptive Conversational Agents: Fine-tuning Language-Models for User Classification to enhance Usability  

---


# Research assistant supervisions

## Ongoing

Yi-Sheng Hsu – Rationalization with Metric-based Readability Control  

Maximilian Dustin Nasert & Christopher Ebert – Data-based Interpretability and Evaluation of Self-Rationalizing LLMs  

## Completed
[Qianli Wang](https://github.com/qiaw99) – Interactive NLP model exploration through dialogue systems  

João Lucas Mendes de Lemos Lins – Instructional explanations  

[Sahil Chopra](https://schopra6.github.io/) – Rationale generation for dialogue-based explanations  

[Ajay Madhavan Ravichandran](https://github.com/aj280192) – Conceptualizing dialogue-based explanations  


---


# Courses
2022-10 - 2023-03 : Explainability in Natural Language Processing @ TU Berlin. Topics: (1) Contrastive Explanations of Text Generation Models. (2) Explainable Fact Checking.  

2021-10 - 2022-03 : MSc/BSc software project @ TU Berlin: Assessing the Quality of Machine-translated Text (with Eleftherios Avramidis & Vivien Macketanz)  

