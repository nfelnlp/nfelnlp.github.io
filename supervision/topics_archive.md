<!-- ORPEx -->
**Analyzing User Behavior in Explanatory Fact Checking Systems**  
How can we measure and mitigate human overreliance on persuasive language in the fact checking domain and explanations generated by LLM-based fact checking systems?
<p style="font-size:smaller;"> 
[1] <a href="https://aclanthology.org/2024.naacl-long.81/">LLMs Help Humans Verify Truthfulness (Si et al., NAACL 2024)</a><br>
[2] <a href="https://dl.acm.org/doi/10.1145/3579605">Explanations Can Reduce Overreliance (Vasconcelos et al., CSCW 2023)</a><br>
[3] <a href="https://doi.org/10.1609/icwsm.v15i1.18072">Explanations to Prevent Overtrust (Mohseni et al., ICWSM 2021)</a><br>
[4] <a href="https://doi.org/10.1002/ail2.49">Explanation Details Affecting Human Performance (Linder et al., Applied AI Letters 2021)</a><br>
[5] <a href="https://arxiv.org/abs/2404.12558">Perception of Explanations in Subjective Decision-Making (Ferguson et al., CHI 2024 TREW)</a><br>
[6] <a href="https://dl.acm.org/doi/10.1145/3630106.3659031">Role of XAI in Collaborative Disinformation Detection (Schmitt et al., FAccT 2024)</a><br>
[7] <a href="https://aclanthology.org/2021.findings-acl.259/">Belief Bias and Explanations (Gonz√°lez et al., ACL 2021 Findings)</a>
</p><br>
