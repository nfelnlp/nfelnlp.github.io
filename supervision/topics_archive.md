---
layout: single
classes: wide
author_profile: true
title: "Archived Topics"
---

<!-- AcAI -->
**Agentic auto-interpretability (with computational constraints)**
Designing small-scale LLM agents with self-testing and self-interpretability tools
<p style="font-size:smaller;">
[1] <a href="https://dl.acm.org/doi/10.5555/3692070.3693872">MAIA (Shaham et al., ICML 2024)</a><br>
[2] <a href="https://openreview.net/forum?id=Sx038qxjek">CRITIC (Gou et al., ICLR 2024)</a><br>
[3] <a href="https://aclanthology.org/2024.naacl-long.110/">Liu et al. (NAACL 2024)</a><br>
[4] <a href="https://arxiv.org/abs/2506.12152">Kim et al. (2025)</a><br>
[5] <a href="https://arxiv.org/abs/2405.00208">Ferrando et al. (2024)</a><br>
</p><br>

<!-- SyClue -->
**Estimating the influence of LLM sycophancy on user interactions**
<p style="font-size:smaller;">
[1] <a href="https://arxiv.org/abs/2505.13995">ELEPHANT (Cheng et al., 2025)</a><br>
[2] <a href="https://aclanthology.org/2024.acl-long.858/">Farm (Xu et al., 2024)</a><br>
[3] <a href="https://arxiv.org/abs/2509.10830">The Siren Song of LLMs (Shi et al., 2025)</a><br>
[4] <a href="https://openreview.net/forum?id=Orvjm9UqH2">Epistemic Alignment (Clark et al., COLM 2025)</a><br>
[5] <a href="https://doi.org/10.1080/10447318.2025.2574511">Don't Be Fooled (Spitzer et al., IJHCI 2025)</a><br>
[6] <a href="https://openreview.net/forum?id=MzM99vV5Rx">IQA-EVAL (Li et al., NeurIPS 2024)</a><br>
</p><br>

<!-- DeLoreason -->
**Explaining Knowledge Conflicts and Factual Errors (of Temporal Generalization) in LLM Generations**  
How can we expose and express knowledge conflicts in LLMs resulting from poor temporal generalization?  
<p style="font-size:smaller;">
[1] <a href="https://aclanthology.org/2024.findings-emnlp.838/">DYNAMICQA (Marjanović et al., EMNLP 2024 Findings)</a><br>
[2] <a href="https://arxiv.org/abs/2310.05189">Survey on Factuality Challenges (Augenstein et al., Nature Machine Intelligence 2024)</a><br>
[3] <a href="https://openreview.net/forum?id=bzs4uPLXvi">Unfaithful Explanations in CoT Prompting (Turpin et al., NeurIPS 2023)</a><br>
[4] <a href="https://aclanthology.org/2023.emnlp-main.751/">Interventions for Explaining Factual Associations (Geva et al., EMNLP 2023)</a><br>
[5] <a href="https://aclanthology.org/2024.acl-long.826/">Self-Bias in LLMs (Xu et al., ACL 2024)</a><br>
[6] <a href="https://aclanthology.org/2024.findings-acl.441">Mismatches between Token Probabilities and LLM Outputs (Wang et al., ACL 2024 Findings)</a><br>
[7] <a href="https://openreview.net/forum?id=ptvV5HGTNN">Resolving Knowledge Conflicts (Wang et al., COLM 2024)</a><br>
[8] <a href="https://openreview.net/forum?id=gfFVATffPd">SAT Probe (Yuksekgonul et al., ICLR 2024)</a><br>
[9] <a href="https://aclanthology.org/2024.naacl-long.46/">MONITOR metric (Wang et al., NAACL 2024)</a>
</p><br>

<!-- InquAIrer -->
**Conversational Model Refinement**  
1. Can we elicit expert human feedback using targeted question generation in a mixed-initiative dialogue setting?
2. Can we use human feedback to natural language explanations to improve the model performance and align it to user preferences?
<p style="font-size:smaller;">
[1] <a href="https://arxiv.org/abs/2103.10415">Compositional Explanations (Yao et al., NeurIPS 2021)</a><br>
[2] <a href="https://aclanthology.org/2024.acl-long.302/">Digital Socrates (Gu et al., ACL 2024)</a><br>
[3] <a href="https://aclanthology.org/2024.naacl-long.168/">Explanation Formats (Malaviya et al., NAACL 2024)</a><br>
[4] <a href="https://aclanthology.org/2022.findings-acl.75/">FeedbackQA (Li et al., ACL 2022 Findings)</a><br>
[5] <a href="https://aclanthology.org/2023.findings-emnlp.791/">Synthesis Step by Step (Wang et al., EMNLP 2023 Findings)</a>
</p><br>

<!-- CircuiTeX -->
**Simplifying Outcomes of Language Model Component Analyses**  
How can results and findings from LM component analysis and mechanistic interpretability studies that are very hard to comprehend for non-experts be simplified and illustrated?  
<p style="font-size:smaller;">
[1] <a href="https://aclanthology.org/2023.blackboxnlp-1.24/">NLEs for Neurons (Huang et al., BlackboxNLP 2023)</a><br>
[2] <a href="https://arxiv.org/abs/2305.09863">Summarize and Score (Singh et al., 2023)</a><br>
[3] <a href="https://aclanthology.org/2023.findings-emnlp.939/">Interpreting the Semantic Flow with VISIT (Katz & Belinkov, EMNLP 2023 Findings)</a><br>
[4] <a href="https://aclanthology.org/2024.emnlp-main.376/">Knowledge-Critical Subnetworks (Bayazit et al., EMNLP 2024)</a><br>
[5] <a href="https://arxiv.org/abs/2310.15213">Function Vectors (Todd et al., ICLR 2024)</a><br>
[6] <a href="https://aclanthology.org/2024.acl-demos.6/">LM Transparency Tool (Tufanov et al., ACL 2024 Demos)</a><br>
[7] <a href="https://arxiv.org/abs/2405.00208">Primer on Component Analysis Methods (Ferrando et al., 2024)</a><br>
[8] <a href="https://aclanthology.org/2024.blackboxnlp-1.30/">Mechanistic? (Saphra & Wiegreffe, BlackboxNLP @ EMNLP 2024)"</a>
</p><br>

<!-- MindMech -->
**The Mindful Mechanic: Interpreting LLMs' Decision-Making in Tool Use**  
API-calling and tool use [1, 2] are expected properties of performant LLM and can often be found in evaluations and benchmarks [3, 4, 5] because the models rely on external knowledge sources and calculations to ensure temporal generalization and factual correctness.
However, it remains unclear what parts of a prompt and which mechanisms within LLMs are responsible for deciding when and which tool or API should be used for the next generation step.
In a comprehensive study across both instruction-tuned and out-of-the-box LLMs, we examine the decision-making in tool use benchmarks with interpretability methods offering information flow routes [6] and feature attributions [7].
<p style="font-size:smaller;">
[1] <a href="https://openreview.net/forum?id=Yacmpz84TH">Toolformer (Schick et al., ICLR 2023)</a><br>
[2] <a href="https://openreview.net/forum?id=HtqnVSCj3q">Chameleon (Lu et al., NeurIPS 2023)</a><br>
[3] <a href="https://openreview.net/forum?id=dHng2O0Jjr">ToolLLM (Qin et al., ICLR 2024)</a><br>
[4] <a href="https://openreview.net/forum?id=Xh1B90iBSR">"What Are Tools Anyway?" Survey (Wang et al., COLM 2024)</a><br>
[5] <a href="https://openreview.net/forum?id=Km2XEjH0I5">TACT dataset (Caciularu et al., NeurIPS 2024 D&B)</a><br>
[6] <a href="https://aclanthology.org/2024.acl-demos.6/">LM Transparency Tool (Tufanov et al., ACL 2024 Demos)</a><br>
[7] <a href="https://aclanthology.org/2023.acl-demo.40/">Inseq (Sarti et al., ACL 2023 Demos)</a>
</p>

<!-- MetricX -->
**Explaining Blind Spots of Model-Based Evaluation Metrics for Text Generation**
<p style="font-size:smaller;">
[1] <a href="https://aclanthology.org/2023.acl-long.674/">Blindspot NLG (He, Zhang et al., ACL 2023)</a><br>
[2] <a href="https://aclanthology.org/2024.findings-acl.80/">AdvEval (Chen et al., ACL 2024 Findings)</a><br>
[3] <a href="https://aclanthology.org/2024.eacl-long.8/">LLM Comparative Assessment (Liusie et al., EACL 2024)</a><br>
[4] <a href="https://www.jmlr.org/papers/v25/22-0416.html">Explainable Evaluation Metrics for MT (Leiter et al., JMLR 2024)</a><br>
[5] <a href="https://arxiv.org/abs/2410.03608">TICKing All the Boxes (Cook et al., 2024)</a><br>
[6] <a href="https://openreview.net/forum?id=xYlJRpzZtsY">ROSCOE (Golovneva et al., ICLR 2023)</a><br>
[7] <a href="https://aclanthology.org/2024.acl-long.60/">RORA (Jiang, Lu et al., ACL 2024)</a>
</p>

<!-- ORPEx -->
**Analyzing User Behavior in Explanatory Fact Checking Systems**  
How can we measure and mitigate human overreliance on persuasive language in the fact checking domain and explanations generated by LLM-based fact checking systems?
<p style="font-size:smaller;">
[1] <a href="https://aclanthology.org/2024.naacl-long.81/">LLMs Help Humans Verify Truthfulness (Si et al., NAACL 2024)</a><br>
[2] <a href="https://dl.acm.org/doi/10.1145/3579605">Explanations Can Reduce Overreliance (Vasconcelos et al., CSCW 2023)</a><br>
[3] <a href="https://doi.org/10.1609/icwsm.v15i1.18072">Explanations to Prevent Overtrust (Mohseni et al., ICWSM 2021)</a><br>
[4] <a href="https://doi.org/10.1002/ail2.49">Explanation Details Affecting Human Performance (Linder et al., Applied AI Letters 2021)</a><br>
[5] <a href="https://arxiv.org/abs/2404.12558">Perception of Explanations in Subjective Decision-Making (Ferguson et al., CHI 2024 TREW)</a><br>
[6] <a href="https://dl.acm.org/doi/10.1145/3630106.3659031">Role of XAI in Collaborative Disinformation Detection (Schmitt et al., FAccT 2024)</a><br>
[7] <a href="https://aclanthology.org/2021.findings-acl.259/">Belief Bias and Explanations (González et al., ACL 2021 Findings)</a>
</p><br>
