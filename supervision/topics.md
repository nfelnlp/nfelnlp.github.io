---
layout: single
classes: wide
author_profile: true
title: "Open Topics"
---

<!-- InquAIrer -->
**Conversational Model Refinement**  
1. Can we elicit expert human feedback using targeted question generation in a mixed-initiative dialogue setting?
2. Can we use human feedback to natural language explanations to improve the model performance and align it to user preferences?
<p style="font-size:smaller;">
[1] <a href="https://arxiv.org/abs/2103.10415">Compositional Explanations (Yao et al., 2021)</a><br>
[2] <a href="https://aclanthology.org/2024.acl-long.302/">Digital Socrates (Gu et al., 2024)</a><br>
[3] <a href="https://aclanthology.org/2024.naacl-long.168/">Explanation Formats (Malaviya et al., 2024)</a><br>
[4] <a href="https://aclanthology.org/2022.findings-acl.75/">FeedbackQA (Li et al., 2022)</a><br>
[5] <a href="https://aclanthology.org/2023.findings-emnlp.791/">Synthesis Step by Step (Wang et al., 2023)</a>
</p><br>

<!-- ORPEx -->
**Analyzing User Behavior in Explanatory Fact Checking Systems**  
How can we measure and mitigate human overreliance on persuasive language in the fact checking domain and explanations generated by LLM-based fact checking systems?
<p style="font-size:smaller;"> 
[1] <a href="https://aclanthology.org/2024.naacl-long.81/">LLMs Help Humans Verify Truthfulness (Si et al., 2024)</a><br>
[2] <a href="https://dl.acm.org/doi/10.1145/3579605">Explanations Can Reduce Overreliance (Vasconcelos et al., 2023)</a><br>
[3] <a href="https://doi.org/10.1609/icwsm.v15i1.18072">Explanations to Prevent Overtrust (Mohseni et al., 2021)</a><br>
[4] <a href="https://doi.org/10.1002/ail2.49">Explanation Details Affecting Human Performance (Linder et al., 2021)</a><br>
[5] <a href="https://arxiv.org/abs/2404.12558">Perception of Explanations in Subjective Decision-Making (Ferguson et al., 2024)</a><br>
[6] <a href="https://dl.acm.org/doi/10.1145/3630106.3659031">Role of XAI in Collaborative Disinformation Detection (Schmitt et al., 2024)</a><br>
[7] <a href="https://aclanthology.org/2021.findings-acl.259/">Belief Bias and Explanations (González et al., 2021)</a>
</p><br>

<!-- DeLoreason -->
**Explaining Knowledge Conflicts and Factual Errors (of Temporal Generalization) in LLM Generations**  
How can we expose and express knowledge conflicts in LLMs resulting from poor temporal generalization?  
<p style="font-size:smaller;">
[1] <a href="https://arxiv.org/abs/2407.17023">DYNAMICQA (Marjanović et al., 2024)</a><br>
[2] <a href="http://arxiv.org/abs/2310.05189">Augenstein et al. (2023)</a><br>
[3] <a href="https://openreview.net/forum?id=bzs4uPLXvi">Turpin et al. (2023)</a><br>
[4] <a href="https://aclanthology.org/2023.emnlp-main.751/">Geva et al. (2023)</a><br>
[5] <a href="https://arxiv.org/abs/2402.11436">Xu et al. (2024)</a><br>
[6] <a href="https://arxiv.org/abs/2402.14499">Wang et al. (2024)</a><br>
[7] <a href="http://arxiv.org/abs/2310.00935">Wang et al. (2023)</a><br>
[8] <a href="https://openreview.net/forum?id=gfFVATffPd">Yuksekgonul et al. (2024)</a><br>
[9] <a href="https://aclanthology.org/2024.naacl-long.46/">Wang et al. (2024)</a>
</p>

<!-- CircuiTeX -->
**Simplifying Outcomes of Language Model Component Analyses**  
How can results and findings from LM component analysis and mechanistic interpretability studies that are very hard to comprehend for non-experts be simplified and illustrated?  
<p style="font-size:smaller;">
[1] <a href="https://aclanthology.org/2023.blackboxnlp-1.24/">Huang et al. (2023)</a><br>
[2] <a href="https://arxiv.org/abs/2305.09863">Singh et al. (2023)</a><br>
[3] <a href="https://aclanthology.org/2023.findings-emnlp.939/">Katz & Belinkov (2023)</a><br>
[4] <a href="https://arxiv.org/abs/2310.03084">Bayazit et al. (2024)</a><br>
[5] <a href="https://arxiv.org/abs/2310.15213">Todd et al. (2024)</a><br>
[6] <a href="https://aclanthology.org/2024.acl-demos.6/">Tufanov et al. (2024)</a><br>
[7] <a href="https://arxiv.org/abs/2405.00208">Ferrando et al. (2024)</a>
</p>
