---
layout: single
classes: wide
author_profile: true
title: "Open Topics"
---

If you are a **Bachelor's or Master's student at TU Berlin** and interested in writing your thesis on one of the following topics, please contact me via mail (see sidebar).  
You should have a **solid background** in and have taken prior courses related to **natural language processing** and/or **machine learning**.  
At the moment, I'm not in the position of supervising PhD students on my own, but I'm always happy to provide consultation on an informal basis!

---

<!-- LyCoS -->
**Verbalizing higher-order functions in language models**
<p style="font-size:smaller;">
[1] <a href="https://aclanthology.org/2025.acl-long.866/">MAPS (Elhelo & Geva, ACL 2025)</a><br>
[2] <a href="https://openreview.net/forum?id=btJUnAPQ7j">PRISM (Kopf et al., NeurIPS 2025)</a><br>
[3] <a href="https://openreview.net/forum?id=niUroX9EOd">LatentQA (Pan et al., ICLR 2026)</a><br>
[4] <a href="https://openreview.net/forum?id=LUsx0chTsL">Talking Heads (Merullo et al., NeurIPS 2024)</a><br>
[5] <a href="https://openreview.net/forum?id=VDWdnaM0Gt">Hou & Castanon (ICML 2023)</a><br>
[6] <a href="https://aclanthology.org/2024.emnlp-main.847/">Layer by Layer (Zhao et al., EMNLP 2024)</a><br>
[7] <a href="https://aclanthology.org/2024.emnlp-main.965/">Information Flow Routes (Ferrando & Voita, EMNLP 2024)</a><br>
[8] <a href="https://openreview.net/forum?id=oP3b5YBFoP">Ikeda et al. (COLM 2025)</a><br>
[9] <a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html">On the Biology of a Large Language Model (Lindsey et al., 2025)</a><br>
</p><br>

<!-- MemiCRea -->
**Measuring the influence of verbatim-memorized content in training data on complex reasoning tasks**
<p style="font-size:smaller;">
[1] <a href="https://aclanthology.org/2024.emnlp-main.598/">Huang et al. (EMNLP 2024)</a><br>
[2] <a href="https://openreview.net/forum?id=TatRHT_1cK">Carlini et al. (ICLR 2023)</a><br>
[3] <a href="https://aclanthology.org/2024.findings-emnlp.212/">Prabhakar et al. (EMNLP 2024 Findings)</a><br>
[4] <a href="https://arxiv.org/abs/2508.02037">STIM (Li, Chen et al., 2025)</a><br>
[5] <a href="https://aclanthology.org/2025.emnlp-main.437/">Reason to Rote (Du et al., EMNLP 2025)</a><br>
[6] <a href="https://arxiv.org/abs/2505.24832">Morris et al. (2025)</a><br>
[7] <a href="https://openreview.net/forum?id=Uic3ojVhXh">ParaPO (Chen et al., COLM 2025)</a><br>
[8] <a href="https://arxiv.org/abs/2507.05578">Survey on Memorization (Xiong et al., 2025)</a><br>
</p><br>

<!-- PaCES -->
**Explaining text simplification of medical terminology**  
<p style="font-size:smaller;">
[1] <a href="https://aclanthology.org/2024.acl-long.459/">FactPICO (Joseph et al., ACL 2024)</a><br>
[2] <a href="https://aclanthology.org/2024.findings-emnlp.737/">README (Yao et al., EMNLP 2024 Findings)</a><br>
[3] <a href="https://aclanthology.org/2024.findings-acl.279/">Two-Pronged Human Evaluation of ChatGPT Self-Correction in Radiology Report Simplification (Yang et al., ACL 2024 Findings)</a><br>
[4] <a href="https://arxiv.org/abs/2406.15963">Effectiveness of ChatGPT in explaining complex medical reports to patients (Sun et al., 2024)</a><br>
[5] <a href="https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2815868">Generative Artificial Intelligence to Transform Inpatient Discharge Summaries to Patient-Friendly Language and Format (Zaretsky et al., JAMA 2024)</a><br>
[6] <a href="https://aclanthology.org/2024.emnlp-main.1051/">Fool Me Once? Contrasting Textual and Visual Explanations in a Clinical Decision-Support Setting (Kayser et al., EMNLP 2024)</a>
[7] <a href="https://doi.org/10.1162/tacl_a_00653">Agrawal & Carpuat (TACL 2024)</a><br>
[8] <a href="https://aclanthology.org/2025.coling-main.452/">Barayan et al. (COLING 2025)</a><br>
[9] <a href="https://dl.acm.org/doi/10.1145/3706598.3713229">Bu√ßinca et al. (CHI 2025)</a><br>
[10] <a href="https://aclanthology.org/2024.emnlp-main.318/">RSA-Control (Wang & Demberg, EMNLP 2024)</a><br>
</p><br>

<!-- MIMe -->
**Tracing biomedical knowledge in LLMs**
<p style="font-size:smaller;">
[1] <a href="https://www.nature.com/articles/s41586-023-06291-2">Large language models encode clinical knowledge (Singhal et al., Nature 2023)</a><br>
[2] <a href="https://proceedings.mlr.press/v259/wu25a.html">DILA: Dictionary Label Attention for Mechanistic Interpretability in High-dimensional Multi-label Medical Coding Prediction (Wu et al., PMLR 2025)</a><br>
[3] <a href="https://aclanthology.org/2025.findings-emnlp.789/">Elucidating Mechanisms of Demographic Bias in LLMs for Healthcare (Ahsan et al., EMNLP 2025 Findings)</a><br>
[4] <a href="https://aclanthology.org/2025.findings-emnlp.487/">Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge in Large Language Models (Vladika et al., EMNLP 2025 Findings)</a><br>
</p><br>


---

[*Topics Archive*](./topics_archive.md)
