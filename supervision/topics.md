---
layout: single
classes: wide
author_profile: true
title: "Open Topics"
---

<!-- MindMech -->
**The Mindful Mechanic: Interpreting LLMs' Decision-Making in Tool Use**  
API-calling and tool use [1, 2] are expected properties of performant LLM and can often be found in evaluations and benchmarks [3, 4, 5] because the models rely on external knowledge sources and calculations to ensure temporal generalization and factual correctness.
However, it remains unclear what parts of a prompt and which mechanisms within LLMs are responsible for deciding when and which tool or API should be used for the next generation step.
In a comprehensive study across both instruction-tuned and out-of-the-box LLMs, we examine the decision-making in tool use benchmarks with interpretability methods offering information flow routes [6] and feature attributions [7].
<p style="font-size:smaller;">
[1] <a href="https://openreview.net/forum?id=Yacmpz84TH">Toolformer (Schick et al., ICLR 2023)</a><br>
[2] <a href="https://openreview.net/forum?id=HtqnVSCj3q">Chameleon (Lu et al., NeurIPS 2023)</a><br>
[3] <a href="https://openreview.net/forum?id=dHng2O0Jjr">ToolLLM (Qin et al., ICLR 2024)</a><br>
[4] <a href="https://openreview.net/forum?id=Xh1B90iBSR">"What Are Tools Anyway?" Survey (Wang et al., COLM 2024)</a><br>
[5] <a href="https://openreview.net/forum?id=Km2XEjH0I5">TACT dataset (Caciularu et al., NeurIPS 2024 D&B)</a><br>
[6] <a href="https://aclanthology.org/2024.acl-demos.6/">LM Transparency Tool (Tufanov et al., ACL 2024 Demos)</a><br>
[7] <a href="https://aclanthology.org/2023.acl-demo.40/">Inseq (Sarti et al., ACL 2023 Demos)</a>
</p>

<!-- DeLoreason -->
**Explaining Knowledge Conflicts and Factual Errors (of Temporal Generalization) in LLM Generations**  
How can we expose and express knowledge conflicts in LLMs resulting from poor temporal generalization?  
<p style="font-size:smaller;">
[1] <a href="https://arxiv.org/abs/2407.17023">DYNAMICQA (MarjanoviÄ‡ et al., EMNLP 2024 Findings)</a><br>
[2] <a href="http://arxiv.org/abs/2310.05189">Survey on Factuality Challenges (Augenstein et al., 2023)</a><br>
[3] <a href="https://openreview.net/forum?id=bzs4uPLXvi">Unfaithful Explanations in CoT Prompting (Turpin et al., NeurIPS 2023)</a><br>
[4] <a href="https://aclanthology.org/2023.emnlp-main.751/">Interventions for Explaining Factual Associations (Geva et al., EMNLP 2023)</a><br>
[5] <a href="https://arxiv.org/abs/2402.11436">Self-Bias in LLMs (Xu et al., 2024)</a><br>
[6] <a href="https://aclanthology.org/2024.findings-acl.441">Mismatches between Token Probabilities and LLM Outputs (Wang et al., ACL 2024 Findings)</a><br>
[7] <a href="http://arxiv.org/abs/2310.00935">Resolving Knowledge Conflicts (Wang et al., COLM 2024)</a><br>
[8] <a href="https://openreview.net/forum?id=gfFVATffPd">SAT Probe (Yuksekgonul et al., ICLR 2024)</a><br>
[9] <a href="https://aclanthology.org/2024.naacl-long.46/">MONITOR metric (Wang et al., NAACL 2024)</a>
</p><br>

<!-- CircuiTeX -->
**Simplifying Outcomes of Language Model Component Analyses**  
How can results and findings from LM component analysis and mechanistic interpretability studies that are very hard to comprehend for non-experts be simplified and illustrated?  
<p style="font-size:smaller;">
[1] <a href="https://aclanthology.org/2023.blackboxnlp-1.24/">NLEs for Neurons (Huang et al., BlackboxNLP 2023)</a><br>
[2] <a href="https://arxiv.org/abs/2305.09863">Summarize and Score (Singh et al., 2023)</a><br>
[3] <a href="https://aclanthology.org/2023.findings-emnlp.939/">Interpreting the Semantic Flow with VISIT (Katz & Belinkov, EMNLP 2023 Findings)</a><br>
[4] <a href="https://arxiv.org/abs/2310.03084">Knowledge-Critical Subnetworks (Bayazit et al., 2024)</a><br>
[5] <a href="https://arxiv.org/abs/2310.15213">Function Vectors (Todd et al., ICLR 2024)</a><br>
[6] <a href="https://aclanthology.org/2024.acl-demos.6/">LM Transparency Tool (Tufanov et al., ACL 2024 Demos)</a><br>
[7] <a href="https://arxiv.org/abs/2405.00208">Primer on Component Analysis Methods (Ferrando et al., 2024)</a>
</p><br>

<!-- InquAIrer -->
**Conversational Model Refinement**  
1. Can we elicit expert human feedback using targeted question generation in a mixed-initiative dialogue setting?
2. Can we use human feedback to natural language explanations to improve the model performance and align it to user preferences?
<p style="font-size:smaller;">
[1] <a href="https://arxiv.org/abs/2103.10415">Compositional Explanations (Yao et al., NeurIPS 2021)</a><br>
[2] <a href="https://aclanthology.org/2024.acl-long.302/">Digital Socrates (Gu et al., ACL 2024)</a><br>
[3] <a href="https://aclanthology.org/2024.naacl-long.168/">Explanation Formats (Malaviya et al., NAACL 2024)</a><br>
[4] <a href="https://aclanthology.org/2022.findings-acl.75/">FeedbackQA (Li et al., ACL 2022 Findings)</a><br>
[5] <a href="https://aclanthology.org/2023.findings-emnlp.791/">Synthesis Step by Step (Wang et al., EMNLP 2023 Findings)</a>
</p><br>
