# Supervision

## Open topics

InquAIrer: Conversational Model Refinement from Eliciting Expert Human Feedback using Targeted Question Generation  
<p style="font-size:smaller;">References: [Yao et al. (2021)](https://arxiv.org/abs/2103.10415); [Gu et al. (2023)](https://arxiv.org/abs/2311.09613); [Malaviya et al. (2023)](https://arxiv.org/abs/2311.09558); [He et al. (2023)](https://aclanthology.org/2023.acl-long.474/)</p>  

Analyzing User Behavior in Fact Checking Systems with Retrieval- and Tool-augmented Generation  
<p style="font-size:smaller;">References: [Si et al. (2023)](https://arxiv.org/abs/2310.12558); [Mohseni et al. (2021)](https://doi.org/10.1609/icwsm.v15i1.18072); [Linder et al. (2021)](https://doi.org/10.1002/ail2.49)</p>  

LLM-based Evaluation of Instructional Explanations on Different Expertise Levels  
<p style="font-size:smaller;">References: [Wachsmuth & Alshomary (2022)](https://aclanthology.org/2022.coling-1.27/); [Kupor et al. (2023)](http://arxiv.org/abs/2311.10749); [Lee et al. (2023)](https://dl.acm.org/doi/10.1145/3544548.3581369); [Rooein et al. (2023)](https://arxiv.org/abs/2312.02065)</p>  

Efficiently Evaluating the Faithfulness of Free-text Rationales  
<p style="font-size:smaller;">References: [Parcalabescu & Frank (2023)](http://arxiv.org/abs/2311.07466); [Larionov et al. (2023)](https://aclanthology.org/2023.findings-emnlp.7/); [Schwarzenberg et al. (2021)](https://aclanthology.org/2021.blackboxnlp-1.17/)</p>  

Explaining Disagreements in Automated Text Simplification Evaluation  
<p style="font-size:smaller;">References: [Jiang et al. (2023)](https://arxiv.org/abs/2310.00752); [Ribeiro et al. (2023)](https://aclanthology.org/2023.emnlp-main.714/); [Wadhwa et al. (2023)](http://arxiv.org/abs/2305.14770); [He et al. (2023)](https://aclanthology.org/2023.acl-long.674/)</p>  

Synthesizing Training Data from Human Feedback to Natural Language Explanations  
<p style="font-size:smaller;">References: [Li et al. (2022)](https://aclanthology.org/2022.findings-acl.75/); [Wang et al. (2023)](https://aclanthology.org/2023.findings-emnlp.791/); [Ye et al. (2022)](https://aclanthology.org/2022.findings-emnlp.269/)</p>  



## Ongoing
Maximilian Bleick (with [Aljoscha Burchardt](https://www.dfki.de/~aburch/)) – BA thesis @ TU Berlin: [An Investigation of LLM Chatbots Concerning the Echo Chamber Effect](https://tu.berlin/index.php?id=246820)  

Yi-Sheng Hsu (with [Sherzod Hakimov](https://sherzod-hakimov.github.io/)) – MSc project @ Uni Potsdam: Engineering LLM-generated Explanations with Metric-based Readability Control  


## Completed
[Qianli Wang](https://github.com/qiaw99) (with [Leonhard Hennig](https://dfki-nlp.github.io/authors/leonhard-hennig/)) – MSC thesis @ TU Berlin: A Singular LLM Is All You Need for Dialogue-based Explanation Regarding NLP Tasks  

Konstantin Biskupski (with [Eleftherios Avramidis](https://github.com/lefterav)) – MSc thesis @ TU Berlin: Quality estimation of machine-translated texts with fine-grained classification of errors  

Kiran Rohra (with [Philippe Thomas](https://github.com/erechtheus)) – MSc thesis @ TU Berlin: Comparative error analysis of biomedical image labelling and captioning models  

[Ajay Madhavan Ravichandran](https://github.com/aj280192) (with [Philippe Thomas](https://github.com/erechtheus)) – MSc thesis @ TU Berlin: Evaluating text quality of generated radiology reports  

Mika Rebensburg (with [Tim Polzehl](https://www.tu.berlin/en/qu/ueber-uns/team-personen/gast-wissenschaftler-partner/dr-tim-polzehl) & [Stefan Hillmann](https://www.tu.berlin/index.php?id=29495)) - BSc thesis @ TU Berlin : Automatic Evaluation of Chatbot Dialogs Using Pre-Trained Language Models in the Customer Support Domain  

Daniel Fernau (with [Tim Polzehl](https://www.tu.berlin/en/qu/ueber-uns/team-personen/gast-wissenschaftler-partner/dr-tim-polzehl) & [Stefan Hillmann](https://www.tu.berlin/index.php?id=29495)) – MSc thesis @ TU Berlin: Towards Adaptive Conversational Agents: Fine-tuning Language-Models for User Classification to enhance Usability  

---


# Research assistant supervisions

## Ongoing

Maximilian Dustin Nasert & Christopher Ebert – Data-based Interpretability and Evaluation of Self-Rationalizing LLMs  

## Completed
[Qianli Wang](https://github.com/qiaw99) – Interactive NLP model exploration through dialogue systems  

João Lucas Mendes de Lemos Lins – Instructional explanations  

[Sahil Chopra](https://schopra6.github.io/) – Rationale generation for dialogue-based explanations  

[Ajay Madhavan Ravichandran](https://github.com/aj280192) – Conceptualizing dialogue-based explanations  


---


# Courses
2022-10 - 2023-03 : Explainability in Natural Language Processing @ TU Berlin. Topics: (1) Contrastive Explanations of Text Generation Models. (2) Explainable Fact Checking.  

2021-10 - 2022-03 : MSc/BSc software project @ TU Berlin: Assessing the Quality of Machine-translated Text (with Eleftherios Avramidis & Vivien Macketanz)  

---

<a href="{{ site.url }}/index">Back to Home</a>
